{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "from azure.ai.ml.entities import ComputeInstance\n",
    "from azure.ai.ml.entities import Environment\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import PipelineJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the variables\n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\")\n",
    "\n",
    "# Connect to ML workspace using InteractiveBrowserCredential\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# Create an MLClient object\n",
    "ml_client = MLClient(credential, subscription_id, resource_group, workspace_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster not found, creating a new one: Operation returned an invalid status 'Not Found'\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "cpu_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    # Try to get the existing compute target\n",
    "    compute_target = ml_client.compute.get(cpu_cluster_name)\n",
    "    print(f\"Found existing cluster: {cpu_cluster_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Cluster not found, creating a new one: {str(e)}\")\n",
    "    # Define the configuration for the new cluster\n",
    "    compute_config = AmlCompute(\n",
    "        name=cpu_cluster_name,\n",
    "        size=\"STANDARD_DS11_V2\",  # VM size\n",
    "        min_instances=0,\n",
    "        max_instances=1\n",
    "    )\n",
    "    # Create the new cluster\n",
    "    compute_target = ml_client.compute.begin_create_or_update(compute_config).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "# Define environment with dependencies\n",
    "myenv = Environment(\n",
    "    name=\"myenv\",\n",
    "    conda_file=\"environment.yml\",  # You can provide a conda YAML file if needed\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",  # Base Docker image\n",
    ")\n",
    "\n",
    "# Optionally save the environment to the workspace\n",
    "# ml_client.environments.create_or_update(myenv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.entities import Model\n",
    "\n",
    "# Load dataset from AML\n",
    "dataset_name = \"german_credit_card_hsg\"\n",
    "dataset_version = \"1\"  # Specify version if needed\n",
    "dataset = ml_client.data.get(name=dataset_name, version=dataset_version)\n",
    "\n",
    "# Load model from AML\n",
    "model_name = \"german-credit-card-hsg\"\n",
    "model_version = \"1\"  # Specify version if needed\n",
    "model = ml_client.models.get(name=model_name, version=model_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/batch_score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/batch_score.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input_data\", type=str, help=\"Path to input dataset\")\n",
    "    parser.add_argument(\"--model_path\", type=str, help=\"Path to input model\")\n",
    "    parser.add_argument(\"--output_data\", type=str, help=\"Path to save output dataset\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(args.input_data)  # Assuming input_data is a CSV file path\n",
    "    df.drop('Sno', axis=1, inplace=True)  # Modify this according to your dataset\n",
    "\n",
    "    # Load model\n",
    "    model = joblib.load(args.model_path)  # Load the model\n",
    "\n",
    "    # Score new data\n",
    "    new_data = df[9:16]  # Take a sample of data\n",
    "    results = model.predict(new_data)\n",
    "    new_data['prediction'] = results\n",
    "\n",
    "    # Save predictions as a new dataset\n",
    "    os.makedirs(args.output_data, exist_ok=True)\n",
    "    output_file = os.path.join(args.output_data, \"predictions.csv\")\n",
    "    new_data.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Warning: the provided asset name 'myenv' will not be used for anonymous registration\n",
      "Warning: the provided asset name 'myenv' will not be used for anonymous registration\n",
      "\u001b[32mUploading scripts (0.0 MBs): 100%|##########| 2098/2098 [00:00<00:00, 7870.94it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>MLOps</td><td>boring_kettle_yyx50506nj</td><td>pipeline</td><td>NotStarted</td><td><a href=\"https://ml.azure.com/runs/boring_kettle_yyx50506nj?wsid=/subscriptions/b5b33273-782e-496a-964f-72acfc982719/resourcegroups/hsg-lesson-rg/workspaces/hsg-lesson-aml&amp;tid=16b3c013-d300-468d-ac64-7eda0820b6d3\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {'input_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x0000014A778424C0>, 'model_path': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x0000014A77842610>}, 'outputs': {}, 'jobs': {}, 'component': PipelineComponent({'latest_version': None, 'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': None, 'base_path': 'c:\\\\Users\\\\sifinell\\\\OneDrive\\\\HSG\\\\MLOps', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x0000014A77865FD0>, 'version': '1', 'schema': None, 'type': 'pipeline', 'display_name': 'batch_pipeline', 'is_deterministic': None, 'inputs': {'input_data': {}, 'model_path': {}}, 'outputs': {}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'batch_job': Command({'parameters': {}, 'init': False, 'name': 'batch_job', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': 'c:\\\\Users\\\\sifinell\\\\OneDrive\\\\HSG\\\\MLOps', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x0000014A77887C10>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <TraceLogger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'input_data': '${{parent.inputs.input_data}}', 'model_path': '${{parent.inputs.model_path}}'}, 'job_outputs': {}, 'inputs': {'input_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x0000014A778A01C0>, 'model_path': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x0000014A77865E80>}, 'outputs': {}, 'component': 'azureml_anonymous:98ef3f19-65cb-4347-bf73-bac151a3f62c', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'a25dabff-c527-49ea-8540-7ccc1060bbd9', 'source': 'CLASS', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 1}, 'job_sources': {'CLASS': 1}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'NotStarted', 'log_files': None, 'name': 'boring_kettle_yyx50506nj', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': '/subscriptions/b5b33273-782e-496a-964f-72acfc982719/resourceGroups/hsg-lesson-rg/providers/Microsoft.MachineLearningServices/workspaces/hsg-lesson-aml/jobs/boring_kettle_yyx50506nj', 'Resource__source_path': '', 'base_path': 'c:\\\\Users\\\\sifinell\\\\OneDrive\\\\HSG\\\\MLOps', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x0000014A77865640>, 'serialize': <msrest.serialization.Serializer object at 0x0000014A77842A00>, 'display_name': 'batch_pipeline', 'experiment_name': 'MLOps', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://switzerlandnorth.api.azureml.ms/mlflow/v1.0/subscriptions/b5b33273-782e-496a-964f-72acfc982719/resourceGroups/hsg-lesson-rg/providers/Microsoft.MachineLearningServices/workspaces/hsg-lesson-aml?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/boring_kettle_yyx50506nj?wsid=/subscriptions/b5b33273-782e-496a-964f-72acfc982719/resourcegroups/hsg-lesson-rg/workspaces/hsg-lesson-aml&tid=16b3c013-d300-468d-ac64-7eda0820b6d3', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml import Input, Output\n",
    "from azure.ai.ml.entities import CommandComponent\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "# Define the component that runs the batch_score.py script\n",
    "batch_scoring_component = CommandComponent(\n",
    "    name=\"batch_scoring_component\",\n",
    "    display_name=\"Batch Scoring Component\",\n",
    "    description=\"Component that runs the batch scoring script\",\n",
    "    environment=myenv,\n",
    "    code=\"./scripts\",  # Path to the folder containing batch_score.py\n",
    "    command=\"python batch_score.py --input_data ${{inputs.input_data}} --model_path ${{inputs.model_path}} --output_data ${{outputs.output_data}}\",\n",
    "    inputs={\n",
    "        \"input_data\": Input(type=\"uri_file\"),\n",
    "        \"model_path\": Input(type=\"uri_file\")\n",
    "    },\n",
    "    outputs={\n",
    "        \"output_data\": Output(type=\"uri_folder\")  # Define an output to store the results\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create a pipeline to execute the component\n",
    "@pipeline(default_compute=cpu_cluster_name)\n",
    "def batch_pipeline(input_data, model_path):\n",
    "    batch_job = batch_scoring_component(input_data=input_data, model_path=model_path)\n",
    "\n",
    "# Provide the required inputs to the pipeline\n",
    "pipeline_job = batch_pipeline(\n",
    "    input_data=Input(type=\"uri_file\", path=dataset.path),  # Pass the dataset\n",
    "    model_path=Input(type=\"uri_file\", path=model.path)  # Pass the model\n",
    ")\n",
    "\n",
    "# Submit the pipeline job\n",
    "ml_client.jobs.create_or_update(pipeline_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish the pipeline\n",
    "#pipeline_job.name = \"pred-ops-concept-test\"\n",
    "#ml_client.jobs.create_or_update(pipeline_job)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
